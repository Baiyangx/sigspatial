\section{Experiments}
\label{sec:exp}
We apply our research method experimentally on a large spatial-temporal dataset collected from IEEE dataport. By comparing the result with real events in Coronanet which records government responses to coronavirus, we evaluate the plausibility of the method. The result shows several points of interest in a specific time period and the details within each topic.
\subsection{Dataset Description and Experiment Setup}
{\bf Dataset}: 

The dataset we used is CORONAVIRUS (COVID-19) GEO-TAGGED TWEETS DATASET form IEEE DataPort. Due to the tweets spreading policy. We can’t access the completed tweets directly. Thus. We use a transfer program named Twarc to batch fetch every tweet in the dataset. Twarc is a python package that is used to export tweets automatically. The main principle is that Twarc will make use of the registration information from twitter developer platform and get the permission from Twitter. Then Twitter can monitor the whole process of getting tweets. This way is completely legal and doesn’t violate the privacy protocols set by Twitter. But it also causes some issues. If the tweets account is banned or the permission has been changed, we can’t access the content anymore. Normally Twitter doesn’t send any warning message but changes the content of tweets to a prompt message. So some filtering process is necessary to ensure that the data set is not contaminated with irrelevant information.

{\bf Metrics}:

We use silhouette factor to evaluate the performance of clustering. For storyline generation, we based on rationality of the time series and topic keywords. 

{\bf Topic mining}:

To evaluate the quality of generated topics, we focus on both the content of the topic itself and the distribution of the topic. 


In the experiment, each topic consists of a series of related sentence vectors. We use high-frequency words in WordCloud to show the content of the topic. Besides, We also output one of the most representative sentences in each topic as supporting evidence. 

The distribution of clusters in the vector space is another indicator of the quality of the results. The projection of vectors in two-dimensional space can intuitively see the distribution of the cluster. In order to verify the temporal relevance of the topic we also use the time label of the topic vector as the third axis to generate a three-dimensional vector distribution map.

{\bf Comparison Models}: 

We propose a combination of lda and bert methods to produce high-quality sentence vectors. To verify the advantages of this approach, we output the clustering results using bert and LDA separately. We make a comprehensive assessment based on the quality of the topics and timelines generated by each method
\subsection{Results and Discussion}
\subsubsection{Evaluation of clustering}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{imgs/silhouette_score.png}
\caption{silhouette score}
\label{fig:silhouette score}
\end{figure}

In Fig~\ref{fig:silhouette score}, we adjust the number of clusters and weight ratio of the vector structure to get the best silhouette score. The quality of the generated storyline will also be a reference. For tweets dataset from May 2020 to August 2021, we found that silhouette score is maximized when the number of clusters is set to 18 and gamma is set to 9. We use this parameter pair for clustering in subsequent experiments.

\begin{table}[]
\begin{tabular}{|c|c|c|c|}
\hline
conditions & gamma = 7 & gamma = 8 & gamma = 9 \\ \hline
k = 16     & 0.028     & 0.025     & -0.005    \\ \hline
k = 18     & -0.007    & 0.017     & 0.040     \\ \hline
k = 20     & -0.006    & 0.008     & 0.032     \\ \hline
\end{tabular}
\end{table}

\subsubsection{Topic content}
For each cluster we get, we focus on keywords and representative tweet within it. Fig~\ref{fig:wordcloud} shows an example. The wordcloud shows high frequencywords in current topic. As a supplement to the topic, we also obtained the central sentence of the cluster in each topic as the most representative tweet in Fig~\ref{fig:center_sentence}. By these methods above, we show what people really care about in this topic.
\begin{figure}
\centering
{
\includegraphics[width=0.4\textwidth]{imgs/lda_bert/Topic0_wordcloud.png}
\caption{wordcloud for one topic}
\label{fig:wordcloud}
}
\hspace{10pt}    
{
\includegraphics[width=0.4\textwidth]{imgs/lda_bert/center_sentence.png}
\caption{centroid tweet set}
\label{fig:center_sentence}
}
\hspace{10pt}
\end{figure}


\subsubsection{Storyline}
Fig~\ref{fig:storyline for lda} shows a result of storyline from LDA-BERT.  

Each node in the graph represents a topic. From the label of nodes, we can get a list of high frequency words and published time. The number attached on the edges stand for a measure for correlation. Nodes with dark backgrounds are connected in series to form a storyline. The connection relationship between nodes reflects the connection between topics.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{imgs/lda_bert/storyline.png}
\caption{slice of storyline for LDA-BERT}
\label{fig:storyline for lda_bert}
\end{figure}


\subsubsection{Model comparision}
For each method, we produce a 2D distribution graph to show the relevance of each cluster in terms of time and content. Fig~\ref{fig:2d_derivation for lda} shows a relatively scattered clustering results. However,in Fig~\ref{fig:storyline for lda} we can see that using lda alone result in too many topics mixed with noise. 

Fig~\ref{fig:2d_derivation for bert} shows poor clustering results that lots of clusters mixed together. It is difficult to reflect the inherent characteristics of each topic.

Fig~\ref{fig:2d_derivation for lda_bert} shows a compromised clustering effect. Some distinct clusters can be found at the edges of the graph. Meanwhile the topic generated from this method shows stronger noise immunity, refer to Fig~\ref{fig:storyline for lda_bert}

\begin{figure}
\centering
{
\includegraphics[width=0.4\textwidth]{imgs/lda/2d_vis.png}
\caption{2d derivation for LDA}
\label{fig:2d_derivation for lda}
}
\hspace{10pt}    %每张图片水平距离
{
\includegraphics[width=0.4\textwidth]{imgs/bert/2d_vis.png}
\caption{2d derivation for BERT}
\label{fig:2d_derivation for bert}
}
\hspace{10pt}
{
\includegraphics[width=0.4\textwidth]{imgs/lda_bert/2d_vis.png}
\caption{2d derivation for LDA-BERT}
\label{fig:2d_derivation for lda_bert}
}
\hspace{10pt}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{imgs/lda/storyline.png}
\caption{slice of storyline for lda}
\label{fig:storyline for lda}
\end{figure}

\subsubsection{Derivation of topic}
Our framework also produce a 3D distribution graph for the topic to show the time correlation, refer to Fig~\ref{fig:3D distribution}, where vertical z-axis stand for time. We can find that some topics are time-sensitive, which may reflect a shift in people's attention.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{imgs/lda_bert/3D_vis.png}
\caption{3D distribution graph}
\label{fig:3D distribution}
\end{figure}